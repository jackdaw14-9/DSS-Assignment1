---
title: "DSS Capstone Project Assignment 1"
author: "chinmoy149"
date: "11 February 2018"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
suppressWarnings(library(dplyr))
suppressWarnings(library(NLP))
suppressWarnings(library(tm))
suppressWarnings(library(wordcloud))
suppressWarnings(library(RColorBrewer))
filepath <- "D:/R/JHU/DSS Capstone Project/Coursera-SwiftKey/final/en_US/"
```

# Peer Graded Assignment 1
## Rubric for the Assignment
The goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm.<br/>Please submit a report on R Pubs (http://rpubs.com/) that explains your exploratory analysis and your goals for the eventual app and algorithm. This document should be concise and explain only the major features of the data you have identified and briefly summarize your plans for creating the prediction algorithm and Shiny app in a way that would be understandable to a non-data scientist manager. You should make use of tables and plots to illustrate important summaries of the data set.<br/>The motivation for this project is to:<ol><li>Demonstrate that you've downloaded the data and have successfully loaded it in.</li><li>Create a basic report of summary statistics about the data sets.</li><li>Report any interesting findings that you amassed so far.</li><li>Get feedback on your plans for creating a prediction algorithm and Shiny app.</li></ol>

## Few Notes on the Data
I have made use of the data given in <strong>en_US</strong> folder. Moreover, I have only used the <strong>blogs</strong> data for the project.<br>The files :- <strong>news</strong> and  <strong>twitter</strong> are given errors -- 
<pre>NEWS dataset
Warning message:
In readLines(news) : incomplete final line found on 'en_US.news.txt'</pre>
<pre>TWITTER dataset
Warning messages:
1: In readLines(twitter) : line 167155 appears to contain an embedded nul
and various other lines also</pre>

## Load the Data
Let us start with loading the data, and selecting a part of it for processing, as it is quite large to be processed at once !!
```{r loading_data, echo = TRUE}
# BLOGS dataset
blogs <- readLines(paste(filepath, "en_US.blogs.txt", sep = ""))
blogs <- blogs[1:9000]
# hard-coded to select the first 9000 lines in blogs
blogs_corpus <- VCorpus (VectorSource (blogs))
rm(blogs) # remove variable no longer needed
```

## First Part of the Assignment
This is the information regarding the files in the ZIP folder
```{r info, echo = FALSE, eval = TRUE, warning = FALSE}
blogs <- "en_US.blogs.txt"
news <- "en_US.news.txt"
twitter <- "en_US.twitter.txt"
datalist <- list(blogs, news, twitter)
info <- function(file_x) {
        name <- strsplit(file_x, '[.,]')[[1]][2]
        file_x <- paste(filepath, file_x, sep = "")
	finfo <- file.info(file_x)[1]/(1024*1024)
	fdata <- readLines(file_x)
	nchars <- lapply(fdata, nchar)
	maxchars <- which.max(nchars)
	nwords <- sum(sapply(strsplit(fdata, "\\s+"), length))
	return (c(name, as.integer(finfo), length(fdata), maxchars, nwords))
}
df <- as.data.frame(lapply(datalist, info))
rownames(df) <- c("FILE", "FILE_SIZE", "LENGHT", "LONGEST_LINE", "TOTAL_WORDS")
colnames(df) <- NULL
print(df)
rm(blogs)
rm(news)
rm(twitter)
rm(df)
```

## Sample Corpus
This code creates the Corpus from the Sample Data, and also removes the unnecesaary variables from the environment, so as to free-up the memory for later operations(later operations are seriously memory-hard)
```{r corpusCreation, echo = FALSE}
corpus <- blogs_corpus %>% tm_map(removePunctuation) %>% tm_map(removeNumbers) %>% tm_map(stripWhitespace) %>% tm_map(content_transformer(tolower)) %>% tm_map(removeWords, stopwords("english")) %>% tm_map(PlainTextDocument)
rm(blogs_corpus)
Unigram <- function(x) {
	unlist(lapply(ngrams(words(x), 1), paste, collapse = " "), use.names = F)
}
Bigram <- function(x) {
	unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = F)
}
Trigram <- function(x) {
	unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = F)
}
blogs_1 <- TermDocumentMatrix(corpus, control = list(tokenize = Unigram))
blogs_2 <- TermDocumentMatrix(corpus, control = list(tokenize = Bigram))
blogs_3 <- TermDocumentMatrix(corpus, control = list(tokenize = Trigram))
```

## Finally
The various Plots, and their wordclouds.<br>Function used -
```{r functions, echo = TRUE}
corpusToDF <- function(theCorpus) {
	m <- as.matrix(theCorpus)
	v <- sort(rowSums(m), decreasing = TRUE)
	return (data.frame(word = names(v), freq = v))
}
```

## Unigram Tokenization
```{r uni, echo = TRUE, warning=FALSE}
d1 <- corpusToDF(blogs_1)
barplot(d1[1:10, ]$freq, las = 2, names.arg = d1[1:10, ]$word, col ="lightblue", main = "Most frequent words", ylab = "Word frequencies")
wordcloud(words = d1$word, freq = d1$freq, min.freq = 40, max.words = 200, random.order = TRUE, rot.per = 0.35,  colors = brewer.pal(8, "Dark2"))
```

## Bigram Tokenization
Gives error, so I did not execute this part of code<pre>Error: cannot allocate vector of size 11.1 Gb
Execution halted</pre>
```{r bi, echo = TRUE, eval = FALSE}
d2 <- corpusToDF(blogs_2)
barplot(d1[1:10, ]$freq, las = 2, names.arg = d1[1:10, ]$word, col ="lightblue", main = "Most frequent words", ylab = "Word frequencies")
wordcloud(words = d1$word, freq = d1$freq, min.freq = 40, max.words = 200, random.order = TRUE, rot.per = 0.35,  colors = brewer.pal(8, "Dark2"))
```

## Trigram Tokenization
Gives error, so I did not execute this snippet,<pre>Error: cannot allocate vector of size 11.8 Gb
Execution halted</pre>
```{r tri, echo = TRUE, eval = FALSE}
d3 <- corpusToDF(blogs_3)
barplot(d1[1:10, ]$freq, las = 2, names.arg = d1[1:10, ]$word, col ="lightblue", main = "Most frequent words", ylab = "Word frequencies")
wordcloud(words = d1$word, freq = d1$freq, min.freq = 40, max.words = 200, random.order = TRUE, rot.per = 0.35,  colors = brewer.pal(8, "Dark2"))
```

I have posted the equivalent code on my <a href="https://github.com/jackdaw14-9/DSS-Assignment1.git">Github Repo</a>.<br/>Regarding any suggestions (or comments) please comment there.